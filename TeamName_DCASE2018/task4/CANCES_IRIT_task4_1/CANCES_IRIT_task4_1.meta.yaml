# Submission information
submission:
  # Submission label
  label: CANCES_IRIT_task4_1

  # Submission name
  name: DCASE2018 task4 Weighted GRU based CRNN

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight, maximum 10 characters
  abbreviation: WGRU & GRU fusion

  # Submission authors in order, mark one of the authors as corresponding author.
  authors:
    # First author
    - lastname: Cances
      firstname: LÃ©o
      email: leo.cances@irit.fr
      corresponding: true                             # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: IRIT
        institute: University Toulouse III - Paul Sabatier, Institue de Recherche en Informatique de Toulouse
        department: Theme 1 - Analyse et synthese de l'information
        location: Toulouse, France

    # Second author
    - lastname: Pellegrini
      firstname: Thomas
      email: thomas.pellegrini@irit.fr                # Contact email address

      # Affiliation information for the author
      affiliation:
        abbreviation: IRIT
        institute: University Toulouse III - Paul Sabatier, Institue de Recherche en Informatique de Toulouse
        department: Theme 1 - Analyse et synthese de l'information
        location: Toulouse, France

	# Third authot
    - lastname: Guyot
      firstname: Patrice
      email: patrice.guyot@irit.fr                # Contact email address

      # Affiliation information for the author
      affiliation:
        abbreviation: INIRITR
        institute: University Toulouse III - Paul Sabatier, Institue de Recherche en Informatique de Toulouse
        department: Theme 1 - Analyse et synthese de l'information
        location: Toulouse, France

# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system. Use general level tags, if possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Audio input
    input_channels: mono                  # e.g. one or multiple [mono, binaural, left, right, mixed, ...]
    input_sampling_rate: 44.1kHz          #

    # Acoustic representation
    acoustic_features: log-mel spectrogram   # e.g one or multiple [MFCC, log-mel energies, spectrogram, CQT, ...]

    # Data augmentation methods
    data_augmentation: !!null             # [time stretching, block mixing, pitch shifting, ...]

    # Machine learning
    # In case using ensemble methods, please specify all methods used (comma separated list).
    machine_learning_method: CRNN         # e.g one or multiple [GMM, HMM, SVM, kNN, MLP, CNN, RNN, CRNN, NMF, random forest, ensemble, ...]

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    ensemble_method_subsystem_count: !!null # [2, 3, 4, 5, ... ]

    # Decision making methods
    decision_making: !!null               # [majority vote, ...]

  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:

    # Total amount of parameters used in the acoustic model. For neural networks, this
    # information is usually given before training process in the network summary.
    # For other than neural networks, if parameter count information is not directly available,
    # try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    total_parameters: 126090

  # URL to the source code of the system [optional]
  source_code: ...

# System results
results:
  # Full results are not mandatory, but for through analysis of the challenge submissions recommended.
  # If you cannot provide all results, also incomplete results can be reported.

  development_dataset:
  # System result for development dataset with the provided cross-validation setup.

  # Overall f1-score (macro average of class-wise F1-scores)
  overall:
    F-score: 16.34%

  # Class-wise accuracies
  class_wise:
    Alarm_bell_ringing:
      F-score: 17.6%
    Blender:
      F-score: 11.6%
    Cat:
      F-score: 0.0%
    Dishes:
      F-score: 0.0%
    Dog:
      F-score: 4.8%
    Electric_shaver_toothbrush:
      F-score: 33.3%
    Frying:
      F-score: 29.5%
    Running_water:
      F-score: 7.1%
    Speech:
      F-score: 19.4%
    Vacuum_cleaner:
      F-score: 40.0% 
