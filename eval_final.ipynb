{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import Metrics\n",
    "import Models\n",
    "import os, tqdm\n",
    "import numpy as np\n",
    "from Encoder import Encoder\n",
    "from Binarizer import Binarizer\n",
    "from datasetGenerator import DCASE2018\n",
    "\n",
    "\n",
    "# evaluate\n",
    "from evaluation_measures import event_based_evaluation\n",
    "from dcase_util.containers import MetaDataContainer\n",
    "\n",
    "from keras.models import model_from_json, Model, load_model\n",
    "from keras.layers import GRU, Bidirectional, Layer, TimeDistributed, Dense, GRUCell\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGRUCell(GRUCell):\n",
    "\n",
    "    def __init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "                 kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None,\n",
    "                 recurrent_constraint=None, bias_constraint=None, dropout=0., recurrent_dropout=0., implementation=1,\n",
    "                 reset_after=False, temporal_weight: float = 0.5, **kwargs):\n",
    "\n",
    "        self.temporal_weight = temporal_weight\n",
    "\n",
    "        super().__init__(units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer,\n",
    "                         bias_initializer, kernel_regularizer, recurrent_regularizer, bias_regularizer,\n",
    "                         kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout,\n",
    "                         implementation, reset_after, **kwargs)\n",
    "        \n",
    "        print(\"Temporal weight : \", self.temporal_weight)\n",
    "\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        h_tm1 = states[0]  # previous memory\n",
    "\n",
    "        # if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
    "        #     self._dropout_mask = _generate_dropout_mask(\n",
    "        #         K.ones_like(inputs),\n",
    "        #         self.dropout,\n",
    "        #         training=training,\n",
    "        #         count=3)\n",
    "        # if (0 < self.recurrent_dropout < 1 and\n",
    "        #         self._recurrent_dropout_mask is None):\n",
    "        #     self._recurrent_dropout_mask = _generate_dropout_mask(\n",
    "        #         K.ones_like(h_tm1),\n",
    "        #         self.recurrent_dropout,\n",
    "        #         training=training,\n",
    "        #         count=3)\n",
    "\n",
    "        # dropout matrices for input units\n",
    "        dp_mask = self._dropout_mask\n",
    "        # dropout matrices for recurrent units\n",
    "        rec_dp_mask = self._recurrent_dropout_mask\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs_z = inputs * dp_mask[0]\n",
    "                inputs_r = inputs * dp_mask[1]\n",
    "                inputs_h = inputs * dp_mask[2]\n",
    "            else:\n",
    "                inputs_z = inputs\n",
    "                inputs_r = inputs\n",
    "                inputs_h = inputs\n",
    "\n",
    "            x_z = K.dot(inputs_z, self.kernel_z)\n",
    "            x_r = K.dot(inputs_r, self.kernel_r)\n",
    "            x_h = K.dot(inputs_h, self.kernel_h)\n",
    "            if self.use_bias:\n",
    "                x_z = K.bias_add(x_z, self.input_bias_z)\n",
    "                x_r = K.bias_add(x_r, self.input_bias_r)\n",
    "                x_h = K.bias_add(x_h, self.input_bias_h)\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1_z = h_tm1 * self.temporal_weight #rec_dp_mask[0]\n",
    "                h_tm1_r = h_tm1 * self.temporal_weight #rec_dp_mask[1]\n",
    "                h_tm1_h = h_tm1 * self.temporal_weight #rec_dp_mask[2]\n",
    "            else:\n",
    "                h_tm1_z = h_tm1 * self.temporal_weight\n",
    "                h_tm1_r = h_tm1 * self.temporal_weight\n",
    "                h_tm1_h = h_tm1 * self.temporal_weight\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            recurrent_z = K.dot(h_tm1_z, self.recurrent_kernel_z)\n",
    "            recurrent_r = K.dot(h_tm1_r, self.recurrent_kernel_r)\n",
    "            if self.reset_after and self.use_bias:\n",
    "                recurrent_z = K.bias_add(recurrent_z, self.recurrent_bias_z)\n",
    "                recurrent_r = K.bias_add(recurrent_r, self.recurrent_bias_r)\n",
    "\n",
    "            z = self.recurrent_activation(x_z + recurrent_z)\n",
    "            r = self.recurrent_activation(x_r + recurrent_r)\n",
    "\n",
    "            # reset gate applied after/before matrix multiplication\n",
    "            if self.reset_after:\n",
    "                recurrent_h = K.dot(h_tm1_h, self.recurrent_kernel_h)\n",
    "                if self.use_bias:\n",
    "                    recurrent_h = K.bias_add(recurrent_h, self.recurrent_bias_h)\n",
    "                recurrent_h = r * recurrent_h\n",
    "            else:\n",
    "                recurrent_h = K.dot(r * h_tm1_h, self.recurrent_kernel_h)\n",
    "\n",
    "            hh = self.activation(x_h + recurrent_h)\n",
    "        else:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs *= dp_mask[0]\n",
    "\n",
    "            # inputs projected by all gate matrices at once\n",
    "            matrix_x = K.dot(inputs, self.kernel)\n",
    "            if self.use_bias:\n",
    "                # biases: bias_z_i, bias_r_i, bias_h_i\n",
    "                matrix_x = K.bias_add(matrix_x, self.input_bias)\n",
    "            x_z = matrix_x[:, :self.units]\n",
    "            x_r = matrix_x[:, self.units: 2 * self.units]\n",
    "            x_h = matrix_x[:, 2 * self.units:]\n",
    "\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1 *= rec_dp_mask[0]\n",
    "\n",
    "            if self.reset_after:\n",
    "                # hidden state projected by all gate matrices at once\n",
    "                matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n",
    "                if self.use_bias:\n",
    "                    matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n",
    "            else:\n",
    "                # hidden state projected separately for update/reset and new\n",
    "                matrix_inner = K.dot(h_tm1,\n",
    "                                     self.recurrent_kernel[:, :2 * self.units])\n",
    "\n",
    "            recurrent_z = matrix_inner[:, :self.units]\n",
    "            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n",
    "\n",
    "            z = self.recurrent_activation(x_z + recurrent_z)\n",
    "            r = self.recurrent_activation(x_r + recurrent_r)\n",
    "\n",
    "            if self.reset_after:\n",
    "                recurrent_h = r * matrix_inner[:, 2 * self.units:]\n",
    "            else:\n",
    "                recurrent_h = K.dot(r * h_tm1,\n",
    "                                    self.recurrent_kernel[:, 2 * self.units:])\n",
    "\n",
    "            hh = self.activation(x_h + recurrent_h)\n",
    "\n",
    "        # previous and candidate state mixed by update gate\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "\n",
    "        if 0 < self.dropout + self.recurrent_dropout:\n",
    "            if training is None:\n",
    "                h._uses_learning_phase = True\n",
    "\n",
    "        return h, [h]\n",
    "\n",
    "\n",
    "class CustomGRU(GRU):\n",
    "\n",
    "    def __init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "                 kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                 kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.,\n",
    "                 recurrent_dropout=0., implementation=1, return_sequences=False, return_state=False, go_backwards=False,\n",
    "                 stateful=False, unroll=False, reset_after=False, temporal_weight: float = 0.5, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(units, activation=activation, recurrent_activation=recurrent_activation,\n",
    "                         use_bias=use_bias, kernel_initializer=kernel_initializer, recurrent_initializer=recurrent_initializer,\n",
    "                         bias_initializer=bias_initializer, kernel_regularizer=kernel_regularizer,\n",
    "                         recurrent_regularizer=recurrent_regularizer, bias_regularizer=bias_regularizer,\n",
    "                         activity_regularizer=activity_regularizer, kernel_constraint=kernel_constraint,\n",
    "                         recurrent_constraint=recurrent_constraint, bias_constraint=bias_constraint,\n",
    "                         dropout=dropout, recurrent_dropout=recurrent_dropout, implementation=implementation,\n",
    "                         return_sequences=return_sequences, return_state=return_state, go_backwards=go_backwards,\n",
    "                         stateful=stateful, unroll=unroll, reset_after=reset_after, **kwargs)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.temporal_weight = temporal_weight\n",
    "\n",
    "        cell = CustomGRUCell(units,\n",
    "                       activation=activation,\n",
    "                       recurrent_activation=recurrent_activation,\n",
    "                       use_bias=use_bias,\n",
    "                       kernel_initializer=kernel_initializer,\n",
    "                       recurrent_initializer=recurrent_initializer,\n",
    "                       bias_initializer=bias_initializer,\n",
    "                       kernel_regularizer=kernel_regularizer,\n",
    "                       recurrent_regularizer=recurrent_regularizer,\n",
    "                       bias_regularizer=bias_regularizer,\n",
    "                       kernel_constraint=kernel_constraint,\n",
    "                       recurrent_constraint=recurrent_constraint,\n",
    "                       bias_constraint=bias_constraint,\n",
    "                       dropout=dropout,\n",
    "                       recurrent_dropout=recurrent_dropout,\n",
    "                       implementation=implementation,\n",
    "                       reset_after=reset_after,\n",
    "                       temporal_weight=temporal_weight)\n",
    "\n",
    "        super(GRU, self).__init__(cell,\n",
    "                                  return_sequences=return_sequences,\n",
    "                                  return_state=return_state,\n",
    "                                  go_backwards=go_backwards,\n",
    "                                  stateful=stateful,\n",
    "                                  unroll=unroll,\n",
    "                                  **kwargs)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config[\"temporal_weight\"] = self.temporal_weight\n",
    "        return config\n",
    "        \n",
    "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
    "        return super().call(inputs, mask, True, initial_state)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load the model and perform prediction\n",
    "def useCustomGRU(dirPath, temporalWeight: float, timeLayer) -> Model:\n",
    "    with open(dirPath + \"_model.json\", \"r\") as modelJsonFile:\n",
    "        model = model_from_json(modelJsonFile.read())\n",
    "    model.load_weights(dirPath + \"_weight.h5py\")\n",
    "    #model.summary()\n",
    "    \n",
    "    #disasemble layers\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Get the trained forward layer from the bidirectional and change it's property\n",
    "    b1 = model.get_layer(\"bidirectional_1\")\n",
    "\n",
    "    x = layers[0].output\n",
    "    for i in range(1, len(layers)):\n",
    "        if layers[i].name == \"bidirectional_1\":\n",
    "            x = Bidirectional(\n",
    "                CustomGRU(units=64, kernel_initializer='glorot_uniform', recurrent_dropout=0.8, dropout=0.0, return_sequences=True, temporal_weight=temporalWeight), name=\"custom_bi\")(x)\n",
    "        elif layers[i].name == \"time_distributed_1\":\n",
    "            x = TimeDistributed( Dense(10, activation=\"sigmoid\"), )(x)\n",
    "        else:\n",
    "            x = layers[i](x)\n",
    "\n",
    "    newModel = Model(input=layers[0].input, output=x)\n",
    "    #newModel.summary()\n",
    "    model.load_weights(dirPath + \"_weight.h5py\")\n",
    "\n",
    "    intermediate_model = Model(input=newModel.input, output=newModel.get_layer(timeLayer).output)\n",
    "    \n",
    "    return intermediate_model\n",
    "\n",
    "# retreive inform,ation about the custom\n",
    "#model = useCustomGRU(0.2)\n",
    "#prediction = model.predict(featTest)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### load the model and perform prediction\n",
    "def useClassic(dirPath):\n",
    "    with open(dirPath + \"_model.json\", \"r\") as modelJsonFile:\n",
    "        model = model_from_json(modelJsonFile.read())\n",
    "    model.load_weights(dirPath + \"_weight.h5py\")\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    intermediate_model = Model(input=model.input, output=model.get_layer(\"time_distributed_1\").output)\n",
    "    #intermediate_model.summary()\n",
    "    \n",
    "    return intermediate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [00:00<00:00, 1464.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation set\n",
    "# Load the test data create the test dataset\n",
    "# load the file list\n",
    "#featTestPath = \"/baie/corpus/DCASE2018/task4/FEATURES_16K/test/mel\"\n",
    "featTestPath = \"/baie/corpus/DCASE2018/task4/FEATURES/test/mel\"\n",
    "featTestList = os.listdir(featTestPath)\n",
    "\n",
    "# load the meta data ----\n",
    "metaPath = \"/baie/corpus/DCASE2018/task4/metadata/test.csv\"\n",
    "with open(metaPath, \"r\") as metaFile:\n",
    "    metadata = metaFile.read().splitlines()[1:]\n",
    "    \n",
    "metadata = [i.split(\"\\t\") for i in metadata]\n",
    "\n",
    "# load the features\n",
    "featTest = []\n",
    "for file in tqdm.tqdm(featTestList):\n",
    "    path = os.path.join(featTestPath, file)\n",
    "    feature = np.load(path)\n",
    "       \n",
    "    # preprocessing\n",
    "    feature = np.expand_dims(feature, axis=-1)\n",
    "    featTest.append(feature)\n",
    "    \n",
    "featTest = np.array(featTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homeLocal/eriador/.miniconda3/envs/dcase/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ti...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal weight :  0.25\n",
      "Temporal weight :  0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homeLocal/eriador/.miniconda3/envs/dcase/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"gl...)`\n",
      "/homeLocal/eriador/.miniconda3/envs/dcase/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ti...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal weight :  0.1\n",
      "Temporal weight :  0.1\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "# remove GAP layer on last network. duplicate and change GRU by WGRU for one\n",
    "#tModel = useClassic(\"results/complete_2\")\n",
    "#t20Model = useCustomGRU(\"results/complete_2\", 0.25, \"time_distributed_1\")\n",
    "#t10Model = useCustomGRU(\"results/complete_2\", 0.10, \"time_distributed_2\")\n",
    "#results/testing/mel_old_noBlank_timeReduction2/oldMel_noBlank_reduce2\n",
    "tModel = useClassic(\"results/testing/mel_old_noBlank_timeReduction2/oldMel_noBlank_reduce2\")\n",
    "t20Model = useCustomGRU(\"results/testing/mel_old_noBlank_timeReduction2/oldMel_noBlank_reduce2\", 0.25, \"time_distributed_1\")\n",
    "t10Model = useCustomGRU(\"results/testing/mel_old_noBlank_timeReduction2/oldMel_noBlank_reduce2\", 0.10, \"time_distributed_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru temporal prediction ...\n",
      "wgru temporal prediction\n",
      "(279, 215, 10)\n",
      "(279, 215, 10)\n",
      "Smooting using the smooth moving average algorithm\n",
      "perform evaluation ...\n"
     ]
    }
   ],
   "source": [
    "print(\"gru temporal prediction ...\")\n",
    "tPrediction = tModel.predict(featTest)\n",
    "print(\"wgru temporal prediction\")\n",
    "t20Prediction = t20Model.predict(featTest)\n",
    "t10Prediction = t10Model.predict(featTest)\n",
    "nbFrame = tPrediction.shape[1]\n",
    "print(tPrediction.shape)\n",
    "\n",
    "# mix the prediction giving the globals prediction\n",
    "w20gru_cls = [0, 2, 1]               # \"impulse\" event well detected by the WGRU\n",
    "w10gru_cls = []\n",
    "gru_cls   = [8, 3, 5, 7, 6, 9, 4]   # \"stationary\" event well detected by the GRU\n",
    "\n",
    "finalTPrediction = []\n",
    "#for i, gp in enumerate(gbPrediction):\n",
    "#    cls = gp.nonzero()[0]\n",
    "for i in range(len(tPrediction)):\n",
    "\n",
    "    curves = np.array([[0]*10 for _ in range(nbFrame)], dtype=np.float32)\n",
    "\n",
    "    # use the WGRU temporal\n",
    "    for c in w20gru_cls:\n",
    "        curves[:,c] = t20Prediction[i][:,c] # use the wgru temporal prediction\n",
    "        \n",
    "    for c in w10gru_cls:\n",
    "        curves[:,c] = t10Prediction[i][:,c] # use the wgru temporal prediction\n",
    "\n",
    "    for c in gru_cls:\n",
    "        curves[:,c] = tPrediction[i][:,c]  # use the classic gru temporal prediction\n",
    "\n",
    "    finalTPrediction.append(curves)\n",
    "finalTPrediction = np.array(finalTPrediction)\n",
    "print(finalTPrediction.shape)\n",
    "\n",
    "encoder = Encoder()\n",
    "segments = encoder.encode(finalTPrediction, method=\"threshold\", smooth=\"smoothMovingAvg\")\n",
    "toEvaluate = encoder.parse(segments, featTestList)\n",
    "\n",
    "print(\"perform evaluation ...\")\n",
    "with open(\"toEvaluate.csv\", \"w\") as f:\n",
    "    f.write(\"filename\\tonset\\toffset\\tevent_label\\n\")\n",
    "    f.write(toEvaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event based metrics (onset-offset)\n",
      "========================================\n",
      "  Evaluated length                  : 2616.88 sec\n",
      "  Evaluated files                   : 288 \n",
      "  Evaluate onset                    : True \n",
      "  Evaluate offset                   : True \n",
      "  T collar                          : 200.00 ms\n",
      "  Offset (length)                   : 20.00 %\n",
      "\n",
      "  Overall metrics (micro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 3.58 %\n",
      "    Precision                       : 2.45 %\n",
      "    Recall                          : 6.62 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 3.48 \n",
      "    Substitution rate               : 0.09 \n",
      "    Deletion rate                   : 0.84 \n",
      "    Insertion rate                  : 2.55 \n",
      "\n",
      "  Class-wise average metrics (macro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 9.28 %\n",
      "    Precision                       : 8.31 %\n",
      "    Recall                          : 12.64 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 3.06 \n",
      "    Deletion rate                   : 0.87 \n",
      "    Insertion rate                  : 2.19 \n",
      "  \n",
      "\n",
      "  Class-wise metrics\n",
      "  ======================================\n",
      "    Event label  | Nref    Nsys  | F        Pre      Rec    | ER       Del      Ins    |\n",
      "    ------------ | -----   ----- | ------   ------   ------ | ------   ------   ------ |\n",
      "    Alarm_bell.. | 112     490   | 0.0%     0.0%     0.0%   | 5.38     1.00     4.38   |\n",
      "    Blender      | 40      85    | 8.0%     5.9%     12.5%  | 2.88     0.88     2.00   |\n",
      "    Cat          | 82      98    | 1.1%     1.0%     1.2%   | 2.17     0.99     1.18   |\n",
      "    Dishes       | 122     62    | 1.1%     1.6%     0.8%   | 1.49     0.99     0.50   |\n",
      "    Dog          | 126     759   | 0.2%     0.1%     0.8%   | 7.01     0.99     6.02   |\n",
      "    Electric_s.. | 28      17    | 22.2%    29.4%    17.9%  | 1.25     0.82     0.43   |\n",
      "    Frying       | 24      51    | 18.7%    13.7%    29.2%  | 2.54     0.71     1.83   |\n",
      "    Running_wa.. | 76      91    | 7.2%     6.6%     7.9%   | 2.04     0.92     1.12   |\n",
      "    Speech       | 260     716   | 3.3%     2.2%     6.2%   | 3.63     0.94     2.69   |\n",
      "    Vacuum_cle.. | 36      80    | 31.0%    22.5%    50.0%  | 2.22     0.50     1.72   |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perso_event_list = MetaDataContainer()\n",
    "perso_event_list.load(filename=\"toEvaluate.csv\")\n",
    "\n",
    "ref_event_list = MetaDataContainer()\n",
    "ref_event_list.load(filename=\"meta/test.csv\")\n",
    "\n",
    "event_based_metric = event_based_evaluation(ref_event_list, perso_event_list)\n",
    "print(event_based_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooting using the smooth moving average algorithm\n",
      "perform evaluation ...\n",
      "Event based metrics (onset-offset)\n",
      "========================================\n",
      "  Evaluated length                  : 2616.88 sec\n",
      "  Evaluated files                   : 288 \n",
      "  Evaluate onset                    : True \n",
      "  Evaluate offset                   : True \n",
      "  T collar                          : 200.00 ms\n",
      "  Offset (length)                   : 20.00 %\n",
      "\n",
      "  Overall metrics (micro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 2.37 %\n",
      "    Precision                       : 1.36 %\n",
      "    Recall                          : 9.05 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 7.03 \n",
      "    Substitution rate               : 0.42 \n",
      "    Deletion rate                   : 0.49 \n",
      "    Insertion rate                  : 6.12 \n",
      "\n",
      "  Class-wise average metrics (macro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 1.76 %\n",
      "    Precision                       : 1.05 %\n",
      "    Recall                          : 10.91 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 12.90 \n",
      "    Deletion rate                   : 0.89 \n",
      "    Insertion rate                  : 12.00 \n",
      "  \n",
      "\n",
      "  Class-wise metrics\n",
      "  ======================================\n",
      "    Event label  | Nref    Nsys  | F        Pre      Rec    | ER       Del      Ins    |\n",
      "    ------------ | -----   ----- | ------   ------   ------ | ------   ------   ------ |\n",
      "    Alarm_bell.. | 112     933   | 0.8%     0.4%     3.6%   | 9.26     0.96     8.29   |\n",
      "    Blender      | 40      387   | 1.9%     1.0%     10.0%  | 10.47    0.90     9.57   |\n",
      "    Cat          | 82      181   | 0.0%     0.0%     0.0%   | 3.21     1.00     2.21   |\n",
      "    Dishes       | 122     444   | 0.0%     0.0%     0.0%   | 4.64     1.00     3.64   |\n",
      "    Dog          | 126     64    | 0.0%     0.0%     0.0%   | 1.51     1.00     0.51   |\n",
      "    Electric_s.. | 28      874   | 0.9%     0.5%     14.3%  | 31.93    0.86     31.07  |\n",
      "    Frying       | 24      797   | 2.4%     1.3%     41.7%  | 33.38    0.58     32.79  |\n",
      "    Running_wa.. | 76      862   | 2.1%     1.2%     13.2%  | 12.08    0.87     11.21  |\n",
      "    Speech       | 260     826   | 8.7%     5.7%     18.1%  | 3.82     0.82     3.00   |\n",
      "    Vacuum_cle.. | 36      642   | 0.9%     0.5%     8.3%   | 18.67    0.92     17.75  |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder()\n",
    "segments = encoder.encode(t10Prediction, method=\"threshold\", smooth=\"smoothMovingAvg\", window_len=19)\n",
    "toEvaluate = encoder.parse(segments, featTestList)\n",
    "\n",
    "print(\"perform evaluation ...\")\n",
    "with open(\"toEvaluate.csv\", \"w\") as f:\n",
    "    f.write(\"filename\\tonset\\toffset\\tevent_label\\n\")\n",
    "    f.write(toEvaluate)\n",
    "    \n",
    "perso_event_list = MetaDataContainer()\n",
    "perso_event_list.load(filename=\"toEvaluate.csv\")\n",
    "\n",
    "ref_event_list = MetaDataContainer()\n",
    "ref_event_list.load(filename=\"meta/test.csv\")\n",
    "\n",
    "event_based_metric = event_based_evaluation(ref_event_list, perso_event_list)\n",
    "print(event_based_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
